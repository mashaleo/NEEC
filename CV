import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import normalize
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer

# Download stopwords if you haven't already
nltk.download('stopwords')

# Load your data (replace 'Reviews.csv' with your actual file path)
data = pd.read_csv('Reviews.csv')

# Filter profiles and perform preprocessing as per your original process
profile_counts = data['ProfileName'].value_counts()
valid_profiles = profile_counts[profile_counts >= 100].index
filtered_df = data[data['ProfileName'].isin(valid_profiles)]

# Classify scores into positive/negative
def classify_score(score):
    if score < 3:
        return 0
    else:
        return 1

filtered_df['Score'] = filtered_df['Score'].map(classify_score)

# Removing duplicates
final = filtered_df.drop_duplicates(subset=["UserId", "ProfileName", "Time", "Text"], keep='first')

# Preprocess the text data
stop = set(stopwords.words('english'))  # Set of stopwords
sno = SnowballStemmer('english')  # Initialize Snowball Stemmer

# Function to clean HTML tags
def cleanhtml(sentence):
    cleanr = re.compile('<.*?>')
    return re.sub(cleanr, ' ', sentence)

# Function to remove punctuation
def cleanpunc(sentence):
    return re.sub(r'[?|!|\'|"|#]', r'', sentence)

# Preprocess the 'Text' column
final_string = []
list_of_sent = []

for sent in final['Text']:
    filtered_sentence = []
    sent = cleanhtml(sent)
    for w in sent.split():
        for cleaned_word in cleanpunc(w).split():
            if cleaned_word.isalpha() and len(cleaned_word) > 2:
                word_lower = cleaned_word.lower()
                if word_lower not in stop:
                    stemmed_word = sno.stem(word_lower)
                    filtered_sentence.append(stemmed_word)
    list_of_sent.append(filtered_sentence)
    final_string.append(" ".join(filtered_sentence))

# Add the cleaned text back into the DataFrame
final['CleanedText'] = final_string

# Step 1: Vectorize the cleaned text using CountVectorizer
count_vect = CountVectorizer()
final_counts = count_vect.fit_transform(final['CleanedText'].values)

# Step 2: Normalize the document-term matrix (optional)
final_counts_normalized = normalize(final_counts)

from sklearn.decomposition import PCA

pca = PCA(n_components=50)  # Reduce to 100 dimensions
final_counts_pca = pca.fit_transform(final_counts_normalized.toarray())

# Step 4: Plot the dendrogram
plt.figure(figsize=(10, 7))
plt.title("Dendrogram for Hierarchical Clustering of Reviews")
plt.xlabel('Sample Index')
plt.ylabel('Distance')

# Create and plot the dendrogram
dendrogram(Z)

# Show the plot
plt.show()

from scipy.cluster.hierarchy import fcluster

max_distance = 15  # Example threshold distance for cutting
clusters = fcluster(Z, max_distance, criterion='distance')

# Alternatively, you can extract a specific number of clusters
clusters = fcluster(Z, t=5, criterion='maxclust')  # Example for 5 clusters

final['Cluster'] = clusters
# View the number of reviews in each cluster
print(final['Cluster'].value_counts())

# Example: Print reviews from a specific cluster
cluster_1_reviews = final[final['Cluster'] == 1]
print(cluster_1_reviews[['Text', 'Score']].head())

explained_variance = np.sum(pca.explained_variance_ratio_)
print(f'Total variance explained by the first 100 components: {explained_variance:.2f}')

from sklearn.metrics import silhouette_score

silhouette_avg = silhouette_score(final_counts_pca, clusters)
print(f'Silhouette Score: {silhouette_avg:.2f}')
