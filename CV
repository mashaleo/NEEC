import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import normalize
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer

# Download stopwords if you haven't already
nltk.download('stopwords')

# Load your data (replace 'Reviews.csv' with your actual file path)
data = pd.read_csv('Reviews.csv')

# Filter profiles and perform preprocessing as per your original process
profile_counts = data['ProfileName'].value_counts()
valid_profiles = profile_counts[profile_counts >= 100].index
filtered_df = data[data['ProfileName'].isin(valid_profiles)]

# Classify scores into positive/negative
def classify_score(score):
    if score < 3:
        return 0
    else:
        return 1

filtered_df['Score'] = filtered_df['Score'].map(classify_score)

# Removing duplicates
final = filtered_df.drop_duplicates(subset=["UserId", "ProfileName", "Time", "Text"], keep='first')

# Preprocess the text data
stop = set(stopwords.words('english'))  # Set of stopwords
sno = SnowballStemmer('english')  # Initialize Snowball Stemmer

# Function to clean HTML tags
def cleanhtml(sentence):
    cleanr = re.compile('<.*?>')
    return re.sub(cleanr, ' ', sentence)

# Function to remove punctuation
def cleanpunc(sentence):
    return re.sub(r'[?|!|\'|"|#]', r'', sentence)

# Preprocess the 'Text' column
final_string = []
list_of_sent = []

for sent in final['Text']:
    filtered_sentence = []
    sent = cleanhtml(sent)
    for w in sent.split():
        for cleaned_word in cleanpunc(w).split():
            if cleaned_word.isalpha() and len(cleaned_word) > 2:
                word_lower = cleaned_word.lower()
                if word_lower not in stop:
                    stemmed_word = sno.stem(word_lower)
                    filtered_sentence.append(stemmed_word)
    list_of_sent.append(filtered_sentence)
    final_string.append(" ".join(filtered_sentence))

# Add the cleaned text back into the DataFrame
final['CleanedText'] = final_string

# Step 1: Vectorize the cleaned text using CountVectorizer
count_vect = CountVectorizer()
final_counts = count_vect.fit_transform(final['CleanedText'].values)

# Step 2: Normalize the document-term matrix (optional)
final_counts_normalized = normalize(final_counts)

from sklearn.decomposition import PCA
pca = PCA(n_components=50)  # Reduce to 50 dimensions
final_counts_pca = pca.fit_transform(final_counts_normalized.toarray())

Z = linkage(final_counts_pca, method='ward')

# Step 4: Plot the dendrogram
plt.figure(figsize=(10, 7))
plt.title("Dendrogram for Hierarchical Clustering of Reviews")
plt.xlabel('Sample Index')
plt.ylabel('Distance')

# Create and plot the dendrogram
dendrogram(Z)

# Show the plot
plt.show()

import matplotlib.pyplot as plt
import numpy as np
from scipy.cluster.hierarchy import fcluster
from sklearn.metrics import pairwise_distances_argmin_min

# Function to calculate WSS (Within-Cluster Sum of Squares)
def calculate_wss(data, clusters):
    wss = 0
    for cluster_num in np.unique(clusters):
        cluster_data = data[clusters == cluster_num]
        centroid = np.mean(cluster_data, axis=0)
        wss += np.sum((cluster_data - centroid) ** 2)
    return wss


# List to store WSS values for each number of clusters
wss_values = []

# Trying different numbers of clusters (e.g., from 2 to 10)
cluster_range = range(2, 11)  # You can extend this range if needed


for k in cluster_range:
    # Create clusters using fcluster with maxclust criterion
    clusters = fcluster(Z, t=k, criterion='maxclust')
    
    # Calculate WSS for the current number of clusters
    wss = calculate_wss(final_counts_pca, clusters)
    
    # Append WSS value to the list
    wss_values.append(wss)
    
    print(f'For {k} clusters, WSS is: {wss:.2f}')



# Plotting WSS vs Number of Clusters (Elbow Method)
plt.figure(figsize=(10, 6))
plt.plot(cluster_range, wss_values, marker='o')
plt.title('Elbow Method for Optimal Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('WSS (Within-Cluster Sum of Squares)')
plt.grid(True)
plt.show()

final['Cluster'] = fcluster(Z, t=10, criterion='maxclust')

for cluster_num in range(1, 11):
    cluster_reviews = final[final['Cluster'] == cluster_num]
    cluster_words = " ".join(cluster_reviews['CleanedText'])
    print(f"Most common words in cluster {cluster_num}:")
    from collections import Counter
    word_counts = Counter(cluster_words.split())
    print(word_counts.most_common(10))  # Top 10 most frequent words

for cluster_num in range(1, 10):
    cluster_reviews = final[final['Cluster'] == cluster_num]
    pos_reviews = cluster_reviews[cluster_reviews['Score'] == 1].shape[0]
    neg_reviews = cluster_reviews[cluster_reviews['Score'] == 0].shape[0]
    print(f"Cluster {cluster_num}: {pos_reviews} positive reviews, {neg_reviews} negative reviews")

final['ReviewLength'] = final['CleanedText'].apply(lambda x: len(x.split()))
for cluster_num in range(1, 5):
    avg_length = final[final['Cluster'] == cluster_num]['ReviewLength'].mean()
    print(f"Average review length in cluster {cluster_num}: {avg_length}")

from sklearn.decomposition import PCA
import seaborn as sns

# Reduce PCA to 2 dimensions for visualization
pca_2d = PCA(n_components=2)
final_counts_2d = pca_2d.fit_transform(final_counts_normalized.toarray())

# Plot the clusters
plt.figure(figsize=(10, 7))
sns.scatterplot(x=final_counts_2d[:, 0], y=final_counts_2d[:, 1], hue=final['Cluster'], palette="viridis")
plt.title("Visualization of 4 Clusters")
plt.show()

for cluster_num in range(1, 5):
    print(f"\nCluster {cluster_num} sample reviews:")
    print(final[final['Cluster'] == cluster_num][['Text', 'Score']].head())

from scipy.cluster.hierarchy import fcluster

#max_distance = 15  # Example threshold distance for cutting
#clusters = fcluster(Z, max_distance, criterion='distance')

# Alternatively, you can extract a specific number of clusters
clusters = fcluster(Z, t=10, criterion='maxclust')  # Example for 5 clusters


# Example: Print reviews from a specific cluster
cluster_1_reviews = final[final['Cluster'] == 1]
print(cluster_1_reviews[['Text', 'Score']].head())

explained_variance = np.sum(pca.explained_variance_ratio_)
print(f'Total variance explained by the first 100 components: {explained_variance:.2f}')

from sklearn.metrics import silhouette_score

silhouette_avg = silhouette_score(final_counts_pca, clusters)
print(f'Silhouette Score: {silhouette_avg:.2f}')
